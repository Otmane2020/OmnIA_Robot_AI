import { useState, useCallback, useRef, useEffect } from 'react';

interface WhisperSTTOptions {
  language?: string;
  model?: string;
  continuous?: boolean;
}

export const useWhisperSTT = (options: WhisperSTTOptions = {}) => {
  const [isRecording, setIsRecording] = useState(false);
  const [isProcessing, setIsProcessing] = useState(false);
  const [transcript, setTranscript] = useState('');
  const [error, setError] = useState<string | null>(null);
  const [isSupported, setIsSupported] = useState(false);
  
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);
  const streamRef = useRef<MediaStream | null>(null);
  const recognitionRef = useRef<any>(null);
  const continuousMode = options.continuous || false;

  // Check browser support
  useEffect(() => {
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    setIsSupported(!!SpeechRecognition);
    
    if (!SpeechRecognition) {
      console.warn('‚ùå Reconnaissance vocale non support√©e par ce navigateur');
    } else {
      setError(null);
      console.log('‚úÖ Reconnaissance vocale support√©e');
    }
  }, []);

  const startRecording = useCallback(async () => {
    try {
      setError(null);
      setTranscript('');
      
      console.log('üé§ Tentative d√©marrage enregistrement...');
      
      // Use browser speech recognition if supported and continuous mode
      if (isSupported && continuousMode) {
        console.log('üé§ Utilisation reconnaissance navigateur');
        return startBrowserRecognition();
      }
      
      console.log('üé§ Utilisation MediaRecorder pour Whisper');
      
      // Demander permission microphone
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          sampleRate: 44100,
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        }
      });
      
      console.log('‚úÖ Acc√®s microphone accord√©');
      streamRef.current = stream;
      audioChunksRef.current = [];
      
      // Cr√©er MediaRecorder
      let mediaRecorder;
      try {
        mediaRecorder = new MediaRecorder(stream, {
          mimeType: 'audio/webm;codecs=opus'
        });
      } catch (error) {
        // Fallback si opus non support√©
        mediaRecorder = new MediaRecorder(stream);
      }
      
      mediaRecorderRef.current = mediaRecorder;
      
      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data);
        }
      };
      
      mediaRecorder.onstop = async () => {
        setIsRecording(false);
        setIsProcessing(true);
        console.log('üîÑ Arr√™t enregistrement, d√©but transcription...');
        
        try {
          // Cr√©er le blob audio
          const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/webm' });
          
          // Envoyer √† Whisper API
          await transcribeWithWhisper(audioBlob);
          console.log('‚úÖ Transcription termin√©e');
          
        } catch (error) {
          console.error('‚ùå Erreur transcription:', error);
          setError('Erreur lors de la transcription');
        } finally {
          setIsProcessing(false);
          // Nettoyer le stream
          if (streamRef.current) {
            streamRef.current.getTracks().forEach(track => track.stop());
            streamRef.current = null;
          }
        }
      };
      
      // D√©marrer l'enregistrement
      mediaRecorder.start();
      setIsRecording(true);
      console.log('üé§ Enregistrement MediaRecorder d√©marr√©');
      console.log('üé§ Enregistrement d√©marr√©');
      
    } catch (error) {
      console.error('‚ùå Erreur d√©marrage enregistrement:', error);
      setError('Impossible d\'acc√©der au microphone');
    }
  }, [isSupported, continuousMode]);

  const startBrowserRecognition = useCallback(() => {
    if (!isSupported) return;

    console.log('üé§ D√©marrage reconnaissance navigateur (continu:', continuousMode, ')...');

    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    const recognition = new SpeechRecognition();
    recognitionRef.current = recognition;

    // Configuration optimis√©e pour conversation
    recognition.continuous = continuousMode; // √âcoute continue selon le mode
    recognition.interimResults = true; // R√©sultats interm√©diaires pour feedback
    recognition.lang = options.language || 'fr-FR';
    recognition.maxAlternatives = 1;
    
    // Am√©liorer la pr√©cision pour le mobilier
    if ('webkitSpeechGrammarList' in window) {
      const grammar = '#JSGF V1.0; grammar mobilier; public <mobilier> = canap√© | table | chaise | lit | armoire | commode | fauteuil | bureau | salon | chambre;';
      const speechRecognitionList = new (window as any).webkitSpeechGrammarList();
      speechRecognitionList.addFromString(grammar, 1);
      recognition.grammars = speechRecognitionList;
    }

    console.log('üé§ Configuration reconnaissance:', { 
      lang: recognition.lang, 
      continuous: recognition.continuous,
      interimResults: recognition.interimResults
    });

    recognition.onstart = () => {
      console.log('üé§ Reconnaissance vocale d√©marr√©e');
      setIsRecording(true);
      setError(null);
    };

    recognition.onresult = (event) => {
      let interimTranscript = '';
      let finalTranscript = '';
      
      for (let i = event.resultIndex; i < event.results.length; i++) {
        const result = event.results[i];
        const transcript = result[0].transcript;
        
        if (result.isFinal) {
          finalTranscript += transcript;
          console.log('üé§ Transcription finale:', transcript);
          console.log('üé§ Confiance:', result[0].confidence);
        } else {
          interimTranscript += transcript;
          console.log('üé§ Transcription interm√©diaire:', transcript);
        }
      }
      
      // Utiliser la transcription finale si disponible, sinon l'interm√©diaire
      const bestTranscript = finalTranscript || interimTranscript;
      
      if (bestTranscript.trim().length > 2) {
        setTranscript(bestTranscript.trim());
        
        // Arr√™ter automatiquement apr√®s transcription finale
        if (finalTranscript && !continuousMode) {
          setTimeout(() => recognition.stop(), 500);
        }
      }
    };

    recognition.onerror = (event) => {
      console.error('‚ùå Erreur reconnaissance vocale:', event.error);
      console.error('‚ùå D√©tails erreur:', event);
      
      if (event.error === 'not-allowed') {
        setError('üé§ Permission microphone refus√©e. Cliquez sur l\'ic√¥ne üîí dans la barre d\'adresse et autorisez le microphone.');
      } else if (event.error === 'no-speech') {
        console.log('‚ö†Ô∏è Aucune parole d√©tect√©e, red√©marrage...');
        setError('üé§ Aucune parole d√©tect√©e. Parlez plus fort ou rapprochez-vous du microphone.');
      } else if (event.error !== 'aborted') {
        setError(`üé§ Erreur reconnaissance: ${event.error}. V√©rifiez votre microphone.`);
      }
      
      setIsRecording(false);
    };

    recognition.onend = () => {
      console.log('üé§ Reconnaissance vocale termin√©e');
      setIsRecording(false);
      recognitionRef.current = null;
      
      // Red√©marrer automatiquement si en mode continu et pas d'erreur
      if (continuousMode && !error) {
        setTimeout(() => {
          if (recognitionRef.current && !isRecording) {
            try {
              recognition.start();
            } catch (error) {
              console.log('Impossible de red√©marrer la reconnaissance');
            }
          }
        }, 1000);
      }
    };

    // Start recognition
    try {
      recognition.start();
      console.log('üé§ Reconnaissance d√©marr√©e avec succ√®s');
    } catch (error) {
      console.error('‚ùå Erreur d√©marrage reconnaissance:', error);
      setError('Erreur d√©marrage reconnaissance vocale');
    }
  }, [isSupported, options.language, continuousMode]);

  const stopRecording = useCallback(() => {
    // Stop browser recognition
    if (recognitionRef.current) {
      console.log('üõë Arr√™t reconnaissance navigateur');
      
      try {
        recognitionRef.current.stop();
        recognitionRef.current = null;
      } catch (error) {
        console.log('Reconnaissance d√©j√† arr√™t√©e');
      }
    }
    
    // Stop media recorder
    if (mediaRecorderRef.current && isRecording) {
      console.log('üõë Arr√™t MediaRecorder');
      
      mediaRecorderRef.current.stop();
      console.log('üõë Enregistrement arr√™t√©');
    }
    
    setIsRecording(false);
  }, [isRecording]);

  const transcribeWithWhisper = async (audioBlob: Blob) => {
    try {
      const supabaseUrl = import.meta.env.VITE_SUPABASE_URL;
      const supabaseKey = import.meta.env.VITE_SUPABASE_ANON_KEY;

      console.log('üîÑ Pr√©paration envoi Whisper...');

      if (!supabaseUrl || !supabaseKey) {
        throw new Error('Supabase non configur√©');
      }

      // Convertir en FormData pour Whisper
      const formData = new FormData();
      formData.append('audio', audioBlob, 'recording.webm');
      formData.append('model', options.model || 'whisper-1');
      formData.append('language', options.language || 'fr');
      
      console.log('üì¶ Taille audio:', audioBlob.size, 'bytes');

      console.log('üîÑ Envoi √† Whisper API...');

      const response = await fetch(`${supabaseUrl}/functions/v1/whisper-stt`, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${supabaseKey}`
        },
        body: formData
      });

      if (!response.ok) {
        const errorData = await response.json();
        console.error('‚ùå Erreur Whisper d√©taill√©e:', errorData);
        throw new Error(errorData.error || 'Erreur Whisper API');
      }

      const result = await response.json();
      const transcribedText = result.text || '';
      
      console.log('‚úÖ Transcription Whisper:', transcribedText);
      setTranscript(transcribedText);
      setError(null);
      
      return transcribedText;
      
    } catch (error) {
      console.error('‚ùå Erreur Whisper:', error);
      throw error;
    }
  };

  const reset = useCallback(() => {
    setTranscript('');
    setError(null);
    
    console.log('üîÑ Reset STT');
    setIsRecording(false);
    setIsProcessing(false);
    
    // Stop browser recognition
    if (recognitionRef.current) {
      try {
        recognitionRef.current.stop();
        console.log('üõë Reconnaissance arr√™t√©e lors du reset');
        recognitionRef.current = null;
      } catch (error) {
        console.log('Reconnaissance d√©j√† arr√™t√©e');
      }
    }
    
    // Nettoyer les ressources
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop());
      streamRef.current = null;
    }
    
    if (mediaRecorderRef.current) {
      mediaRecorderRef.current = null;
    }
    
    audioChunksRef.current = [];
  }, []);

  return {
    isRecording,
    isProcessing,
    isSupported,
    transcript,
    error,
    startRecording,
    stopRecording,
    reset
  };
};